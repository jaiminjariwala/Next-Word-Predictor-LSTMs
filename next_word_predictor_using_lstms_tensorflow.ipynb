{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTGvqEN8bRf1itpuRiq766",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaiminjariwala/Next-Word-Predictor-LSTMs/blob/main/next_word_predictor_using_lstms_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY0Xyl29TJMn",
        "outputId": "873db971-644e-4465-9a42-56a60066e682"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.6)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.0.5)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (0.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bvaYb0aUR4Ec"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"What is a Large Language Model (LLM)?\n",
        "A large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. These models are capable of generating human-like text and performing various natural language processing tasks.\n",
        "\n",
        "In contrast, the definition of a language model refers to the concept of assigning probabilities to sequences of words, based on the analysis of text corpora. A language model can be of varying complexity, from simple n-gram models to more sophisticated neural network models. However, the term “large language model” usually refers to models that use deep learning techniques and have a large number of parameters, which can range from millions to billions. These models can capture complex patterns in language and produce text that is often indistinguishable from that written by humans.\n",
        "\n",
        "How a Large Language Model (LLM) Is Built?\n",
        "A large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an API or web interface. These models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. By analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
        "ChatGPT’s GPT-3, a large language model, was trained on massive amounts of internet text data, allowing it to understand various languages and possess knowledge of diverse topics. As a result, it can produce text in multiple styles. While its capabilities, including translation, text summarization, and question-answering, may seem impressive, they are not surprising, given that these functions operate using special “grammars” that match up with prompts.\n",
        "\n",
        "How do large language models work?\n",
        "Large language models like GPT-3 (Generative Pre-trained Transformer 3) work based on a transformer architecture. Here’s a simplified explanation of how they Work:\n",
        "\n",
        "Learning from Lots of Text: These models start by reading a massive amount of text from the internet. It’s like learning from a giant library of information.\n",
        "Innovative Architecture: They use a unique structure called a transformer, which helps them understand and remember lots of information.\n",
        "Breaking Down Words: They look at sentences in smaller parts, like breaking words into pieces. This helps them work with language more efficiently.\n",
        "Understanding Words in Sentences: Unlike simple programs, these models understand individual words and how words relate to each other in a sentence. They get the whole picture.\n",
        "Getting Specialized: After the general learning, they can be trained more on specific topics to get good at certain things, like answering questions or writing about particular subjects.\n",
        "Doing Tasks: When you give them a prompt (a question or instruction), they use what they’ve learned to respond. It’s like having an intelligent assistant that can understand and generate text.\n",
        "Difference Between Large Language Models and Generative AI:\n",
        "Generative AI is like a big playground with lots of different toys for making new things. It can create poems, music, pictures, even invent new stuff!\n",
        "\n",
        "Large Language Models are like the best word builders in that playground. They’re really good at using words to make stories, translate languages, answer questions, and even write code!\n",
        "\n",
        "So, generative AI is the whole playground, and LLMs are the language experts in that playground.\n",
        "\n",
        "General Architecture\n",
        "The architecture of Large Language Model primarily consists of multiple layers of neural networks, like recurrent layers, feedforward layers, embedding layers, and attention layers. These layers work together to process the input text and generate output predictions.\n",
        "\n",
        "The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.\n",
        "The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.\n",
        "The recurrent layers of LLMs are designed to interpret information from the input text in sequence. These layers maintain a hidden state that is updated at each time step, allowing the model to capture the dependencies between words in a sentence.\n",
        "The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This mechanism helps the model attend to the input text’s most relevant parts and generate more accurate predictions.\n",
        "Examples of LLMs\n",
        "Let’s take a look at some popular large language models(LLM):\n",
        "\n",
        "GPT-3 (Generative Pre-trained Transformer 3) – This is one of the largest Large Language Models developed by OpenAI. It has 175 billion parameters and can perform many tasks, including text generation, translation, and summarization.\n",
        "BERT (Bidirectional Encoder Representations from Transformers) – Developed by Google, BERT is another popular LLM that has been trained on a massive corpus of text data. It can understand the context of a sentence and generate meaningful responses to questions.\n",
        "XLNet – This LLM developed by Carnegie Mellon University and Google uses a novel approach to language modeling called “permutation language modeling.” It has achieved state-of-the-art performance on language tasks, including language generation and question answering.\n",
        "T5 (Text-to-Text Transfer Transformer) – T5, developed by Google, is trained on a variety of language tasks and can perform text-to-text transformations, like translating text to another language, creating a summary, and question answering.\n",
        "RoBERTa (Robustly Optimized BERT Pretraining Approach) – Developed by Facebook AI Research, RoBERTa is an improved BERT version that performs better on several language tasks.\n",
        "Open Source Large Language Model(LLM)\n",
        "The availability of open-source LLMs has revolutionized the field of natural language processing, making it easier for researchers, developers, and businesses to build applications that leverage the power of these models to build products at scale for free. One such example is Bloom. It is the first multilingual Large Language Model (LLM) trained in complete transparency by the largest collaboration of AI researchers ever involved in a single research project.\n",
        "\n",
        "With its 176 billion parameters (larger than OpenAI’s GPT-3), BLOOM can generate text in 46 natural languages and 13 programming languages. It is trained on 1.6TB of text data, 320 times the complete works of Shakespeare.\n",
        "Future Implications of LLMs\n",
        "In recent years, there has been specific interest in large language model (LLMs) like GPT-3, and chatbots like ChatGPT, which can generate natural language text that has very little difference from that written by humans. While LLMs have seen a breakthrough in the field of artificial intelligence (AI), there are concerns about their impact on job markets, communication, and society.\n",
        "\n",
        "One major concern about LLMs is their potential to disrupt job markets. Large Language Model, with time, will be able to perform tasks by replacing humans like legal documents and drafts, customer support chatbots, writing news blogs, etc. This could lead to job losses for those whose work can be easily automated.\n",
        "\n",
        "However, it is important to note that LLMs are not a replacement for human workers. They are simply a tool that can help people to be more productive and efficient in their work. While some jobs may be automated, new jobs will also be created as a result of the increased efficiency and productivity enabled by LLMs. For example, businesses may be able to create new products or services that were previously too time-consuming or expensive to develop.\n",
        "\n",
        "LLMs have the potential to impact society in several ways. For example, LLMs could be used to create personalized education or healthcare plans, leading to better patient and student outcomes. LLMs can be used to help businesses and governments make better decisions by analyzing large amounts of data and generating insights.\n",
        "\n",
        "Conclusion\n",
        "Large Language Model (LLMs) have revolutionized the field of natural language processing, allowing for new advancements in text generation and understanding. LLMs can learn from big data, understand its context and entities, and answer user queries. This makes them a great alternative for regular usage in various tasks in several industries. However, there are concerns about the ethical implications and potential biases associated with these models. It is important to approach LLMs with a critical eye and evaluate their impact on society. With careful use and continued development, LLMs have the potential to bring about positive changes in many domains, but we should be aware of their limitations and ethical implications.\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "Large Language Models (LLMs) can understand complex sentences, understand relationships between entities and user intent, and generate new text that is coherent and grammatically correct\n",
        "The article explores the architecture of some LLMs, including embedding, feedforward, recurrent, and attention layers.\n",
        "The article discusses some of the popular LLMs like BERT, BERT, Bloom, and GPT3 and the availability of open-source LLMs.\n",
        "Hugging Face APIs can be helpful for users to generate text using LLMs like Bart-large-CNN, Roberta, Bloom, and Bart-large-CNN.\n",
        "LLMs are expected to revolutionize certain domains in the job market, communication, and society in the future.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# let's perform some **preprocessing** on text..."
      ],
      "metadata": {
        "id": "-r9xO4jPDtGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Lowercasing the text\n",
        "lowercase_text = text.lower()\n",
        "\n",
        "# Removing extra newlines between sentences\n",
        "cleaned_text = re.sub(r'\\n\\s*\\n', '\\n', lowercase_text)\n",
        "\n",
        "print(\"Lowercased Text with Extra Newlines Removed:\\n\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiRKPHT9__08",
        "outputId": "51500693-9a5f-4654-cd6e-bbdeb09bf601"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased Text with Extra Newlines Removed:\n",
            "\n",
            "what is a large language model (llm)?\n",
            "a large language model is an advanced type of language model that is trained using deep learning techniques on massive amounts of text data. these models are capable of generating human-like text and performing various natural language processing tasks.\n",
            "in contrast, the definition of a language model refers to the concept of assigning probabilities to sequences of words, based on the analysis of text corpora. a language model can be of varying complexity, from simple n-gram models to more sophisticated neural network models. however, the term “large language model” usually refers to models that use deep learning techniques and have a large number of parameters, which can range from millions to billions. these models can capture complex patterns in language and produce text that is often indistinguishable from that written by humans.\n",
            "how a large language model (llm) is built?\n",
            "a large-scale transformer model known as a “large language model” is typically too massive to run on a single computer and is, therefore, provided as a service over an api or web interface. these models are trained on vast amounts of text data from sources such as books, articles, websites, and numerous other forms of written content. by analyzing the statistical relationships between words, phrases, and sentences through this training process, the models can generate coherent and contextually relevant responses to prompts or queries.\n",
            "chatgpt’s gpt-3, a large language model, was trained on massive amounts of internet text data, allowing it to understand various languages and possess knowledge of diverse topics. as a result, it can produce text in multiple styles. while its capabilities, including translation, text summarization, and question-answering, may seem impressive, they are not surprising, given that these functions operate using special “grammars” that match up with prompts.\n",
            "how do large language models work?\n",
            "large language models like gpt-3 (generative pre-trained transformer 3) work based on a transformer architecture. here’s a simplified explanation of how they work:\n",
            "learning from lots of text: these models start by reading a massive amount of text from the internet. it’s like learning from a giant library of information.\n",
            "innovative architecture: they use a unique structure called a transformer, which helps them understand and remember lots of information.\n",
            "breaking down words: they look at sentences in smaller parts, like breaking words into pieces. this helps them work with language more efficiently.\n",
            "understanding words in sentences: unlike simple programs, these models understand individual words and how words relate to each other in a sentence. they get the whole picture.\n",
            "getting specialized: after the general learning, they can be trained more on specific topics to get good at certain things, like answering questions or writing about particular subjects.\n",
            "doing tasks: when you give them a prompt (a question or instruction), they use what they’ve learned to respond. it’s like having an intelligent assistant that can understand and generate text.\n",
            "difference between large language models and generative ai:\n",
            "generative ai is like a big playground with lots of different toys for making new things. it can create poems, music, pictures, even invent new stuff!\n",
            "large language models are like the best word builders in that playground. they’re really good at using words to make stories, translate languages, answer questions, and even write code!\n",
            "so, generative ai is the whole playground, and llms are the language experts in that playground.\n",
            "general architecture\n",
            "the architecture of large language model primarily consists of multiple layers of neural networks, like recurrent layers, feedforward layers, embedding layers, and attention layers. these layers work together to process the input text and generate output predictions.\n",
            "the embedding layer converts each word in the input text into a high-dimensional vector representation. these embeddings capture semantic and syntactic information about the words and help the model to understand the context.\n",
            "the feedforward layers of large language models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. these layers help the model learn higher-level abstractions from the input text.\n",
            "the recurrent layers of llms are designed to interpret information from the input text in sequence. these layers maintain a hidden state that is updated at each time step, allowing the model to capture the dependencies between words in a sentence.\n",
            "the attention mechanism is another important part of llms, which allows the model to focus selectively on different parts of the input text. this mechanism helps the model attend to the input text’s most relevant parts and generate more accurate predictions.\n",
            "examples of llms\n",
            "let’s take a look at some popular large language models(llm):\n",
            "gpt-3 (generative pre-trained transformer 3) – this is one of the largest large language models developed by openai. it has 175 billion parameters and can perform many tasks, including text generation, translation, and summarization.\n",
            "bert (bidirectional encoder representations from transformers) – developed by google, bert is another popular llm that has been trained on a massive corpus of text data. it can understand the context of a sentence and generate meaningful responses to questions.\n",
            "xlnet – this llm developed by carnegie mellon university and google uses a novel approach to language modeling called “permutation language modeling.” it has achieved state-of-the-art performance on language tasks, including language generation and question answering.\n",
            "t5 (text-to-text transfer transformer) – t5, developed by google, is trained on a variety of language tasks and can perform text-to-text transformations, like translating text to another language, creating a summary, and question answering.\n",
            "roberta (robustly optimized bert pretraining approach) – developed by facebook ai research, roberta is an improved bert version that performs better on several language tasks.\n",
            "open source large language model(llm)\n",
            "the availability of open-source llms has revolutionized the field of natural language processing, making it easier for researchers, developers, and businesses to build applications that leverage the power of these models to build products at scale for free. one such example is bloom. it is the first multilingual large language model (llm) trained in complete transparency by the largest collaboration of ai researchers ever involved in a single research project.\n",
            "with its 176 billion parameters (larger than openai’s gpt-3), bloom can generate text in 46 natural languages and 13 programming languages. it is trained on 1.6tb of text data, 320 times the complete works of shakespeare.\n",
            "future implications of llms\n",
            "in recent years, there has been specific interest in large language model (llms) like gpt-3, and chatbots like chatgpt, which can generate natural language text that has very little difference from that written by humans. while llms have seen a breakthrough in the field of artificial intelligence (ai), there are concerns about their impact on job markets, communication, and society.\n",
            "one major concern about llms is their potential to disrupt job markets. large language model, with time, will be able to perform tasks by replacing humans like legal documents and drafts, customer support chatbots, writing news blogs, etc. this could lead to job losses for those whose work can be easily automated.\n",
            "however, it is important to note that llms are not a replacement for human workers. they are simply a tool that can help people to be more productive and efficient in their work. while some jobs may be automated, new jobs will also be created as a result of the increased efficiency and productivity enabled by llms. for example, businesses may be able to create new products or services that were previously too time-consuming or expensive to develop.\n",
            "llms have the potential to impact society in several ways. for example, llms could be used to create personalized education or healthcare plans, leading to better patient and student outcomes. llms can be used to help businesses and governments make better decisions by analyzing large amounts of data and generating insights.\n",
            "conclusion\n",
            "large language model (llms) have revolutionized the field of natural language processing, allowing for new advancements in text generation and understanding. llms can learn from big data, understand its context and entities, and answer user queries. this makes them a great alternative for regular usage in various tasks in several industries. however, there are concerns about the ethical implications and potential biases associated with these models. it is important to approach llms with a critical eye and evaluate their impact on society. with careful use and continued development, llms have the potential to bring about positive changes in many domains, but we should be aware of their limitations and ethical implications.\n",
            "key takeaways:\n",
            "large language models (llms) can understand complex sentences, understand relationships between entities and user intent, and generate new text that is coherent and grammatically correct\n",
            "the article explores the architecture of some llms, including embedding, feedforward, recurrent, and attention layers.\n",
            "the article discusses some of the popular llms like bert, bert, bloom, and gpt3 and the availability of open-source llms.\n",
            "hugging face apis can be helpful for users to generate text using llms like bart-large-cnn, roberta, bloom, and bart-large-cnn.\n",
            "llms are expected to revolutionize certain domains in the job market, communication, and society in the future.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "abuJfZL_SKSp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])"
      ],
      "metadata": {
        "id": "1bdhO7mOSau3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# to see, what numbers are allocated to what words, there's an attribute name \"word_index\""
      ],
      "metadata": {
        "id": "hxwQ5qIySn1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKv_lSQKSg9U",
        "outputId": "a43c6e09-bd2e-4b85-c1dc-de2b145dc925"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'of': 3,\n",
              " 'to': 4,\n",
              " 'a': 5,\n",
              " 'language': 6,\n",
              " 'text': 7,\n",
              " 'in': 8,\n",
              " 'llms': 9,\n",
              " 'large': 10,\n",
              " 'is': 11,\n",
              " 'that': 12,\n",
              " 'model': 13,\n",
              " 'models': 14,\n",
              " 'can': 15,\n",
              " 'on': 16,\n",
              " 'like': 17,\n",
              " 'by': 18,\n",
              " 'these': 19,\n",
              " 'be': 20,\n",
              " 'from': 21,\n",
              " 'layers': 22,\n",
              " 'are': 23,\n",
              " 'it': 24,\n",
              " 'trained': 25,\n",
              " 'words': 26,\n",
              " 'for': 27,\n",
              " 'generate': 28,\n",
              " 'understand': 29,\n",
              " 'tasks': 30,\n",
              " 'they': 31,\n",
              " 'with': 32,\n",
              " 'llm': 33,\n",
              " 'data': 34,\n",
              " 'or': 35,\n",
              " 'this': 36,\n",
              " '3': 37,\n",
              " 'work': 38,\n",
              " 'input': 39,\n",
              " 'have': 40,\n",
              " 'transformer': 41,\n",
              " 'at': 42,\n",
              " 'about': 43,\n",
              " 'ai': 44,\n",
              " 'new': 45,\n",
              " 'has': 46,\n",
              " 'bert': 47,\n",
              " 'learning': 48,\n",
              " 'massive': 49,\n",
              " 'natural': 50,\n",
              " 'more': 51,\n",
              " 'as': 52,\n",
              " 'gpt': 53,\n",
              " 'generative': 54,\n",
              " 'architecture': 55,\n",
              " '–': 56,\n",
              " 'developed': 57,\n",
              " 'their': 58,\n",
              " 'an': 59,\n",
              " 'using': 60,\n",
              " 'amounts': 61,\n",
              " 'use': 62,\n",
              " 'which': 63,\n",
              " 'how': 64,\n",
              " 'between': 65,\n",
              " 'sentences': 66,\n",
              " 'languages': 67,\n",
              " 'including': 68,\n",
              " 'question': 69,\n",
              " 'answering': 70,\n",
              " 'information': 71,\n",
              " 'them': 72,\n",
              " 'playground': 73,\n",
              " 'help': 74,\n",
              " 'some': 75,\n",
              " 'bloom': 76,\n",
              " 'job': 77,\n",
              " 'society': 78,\n",
              " 'potential': 79,\n",
              " 'various': 80,\n",
              " 'processing': 81,\n",
              " 'however': 82,\n",
              " 'parameters': 83,\n",
              " 'capture': 84,\n",
              " 'written': 85,\n",
              " 'humans': 86,\n",
              " 'allowing': 87,\n",
              " 'multiple': 88,\n",
              " 'while': 89,\n",
              " 'its': 90,\n",
              " 'may': 91,\n",
              " 'lots': 92,\n",
              " 'helps': 93,\n",
              " 'parts': 94,\n",
              " 'each': 95,\n",
              " 'sentence': 96,\n",
              " 'questions': 97,\n",
              " 'create': 98,\n",
              " 'recurrent': 99,\n",
              " 'feedforward': 100,\n",
              " 'embedding': 101,\n",
              " 'attention': 102,\n",
              " 'context': 103,\n",
              " 'time': 104,\n",
              " 'another': 105,\n",
              " 'important': 106,\n",
              " 'popular': 107,\n",
              " 'one': 108,\n",
              " 'perform': 109,\n",
              " 'generation': 110,\n",
              " 'google': 111,\n",
              " 'approach': 112,\n",
              " 'roberta': 113,\n",
              " 'better': 114,\n",
              " 'several': 115,\n",
              " 'open': 116,\n",
              " 'source': 117,\n",
              " 'field': 118,\n",
              " 'businesses': 119,\n",
              " 'example': 120,\n",
              " 'implications': 121,\n",
              " 'there': 122,\n",
              " 'impact': 123,\n",
              " 'what': 124,\n",
              " 'deep': 125,\n",
              " 'techniques': 126,\n",
              " 'generating': 127,\n",
              " 'human': 128,\n",
              " 'refers': 129,\n",
              " 'based': 130,\n",
              " 'simple': 131,\n",
              " 'neural': 132,\n",
              " '“large': 133,\n",
              " 'model”': 134,\n",
              " 'complex': 135,\n",
              " 'produce': 136,\n",
              " 'scale': 137,\n",
              " 'too': 138,\n",
              " 'single': 139,\n",
              " 'such': 140,\n",
              " 'other': 141,\n",
              " 'analyzing': 142,\n",
              " 'relationships': 143,\n",
              " 'process': 144,\n",
              " 'coherent': 145,\n",
              " 'relevant': 146,\n",
              " 'responses': 147,\n",
              " 'prompts': 148,\n",
              " 'queries': 149,\n",
              " 'internet': 150,\n",
              " 'topics': 151,\n",
              " 'result': 152,\n",
              " 'translation': 153,\n",
              " 'summarization': 154,\n",
              " 'not': 155,\n",
              " 'pre': 156,\n",
              " 'it’s': 157,\n",
              " 'called': 158,\n",
              " 'breaking': 159,\n",
              " 'look': 160,\n",
              " 'into': 161,\n",
              " 'understanding': 162,\n",
              " 'get': 163,\n",
              " 'whole': 164,\n",
              " 'general': 165,\n",
              " 'specific': 166,\n",
              " 'good': 167,\n",
              " 'certain': 168,\n",
              " 'things': 169,\n",
              " 'writing': 170,\n",
              " 'difference': 171,\n",
              " 'big': 172,\n",
              " 'different': 173,\n",
              " 'making': 174,\n",
              " 'even': 175,\n",
              " 'word': 176,\n",
              " 'make': 177,\n",
              " 'answer': 178,\n",
              " 'predictions': 179,\n",
              " 'embeddings': 180,\n",
              " 'transformations': 181,\n",
              " 'learn': 182,\n",
              " 'state': 183,\n",
              " 'mechanism': 184,\n",
              " 'largest': 185,\n",
              " 'billion': 186,\n",
              " 'many': 187,\n",
              " 'been': 188,\n",
              " 'modeling': 189,\n",
              " 't5': 190,\n",
              " 'research': 191,\n",
              " 'availability': 192,\n",
              " 'revolutionized': 193,\n",
              " 'researchers': 194,\n",
              " 'build': 195,\n",
              " 'products': 196,\n",
              " 'complete': 197,\n",
              " 'future': 198,\n",
              " 'chatbots': 199,\n",
              " 'concerns': 200,\n",
              " 'markets': 201,\n",
              " 'communication': 202,\n",
              " 'will': 203,\n",
              " 'able': 204,\n",
              " 'could': 205,\n",
              " 'automated': 206,\n",
              " 'jobs': 207,\n",
              " 'used': 208,\n",
              " 'entities': 209,\n",
              " 'user': 210,\n",
              " 'ethical': 211,\n",
              " 'domains': 212,\n",
              " 'article': 213,\n",
              " 'bart': 214,\n",
              " 'cnn': 215,\n",
              " 'advanced': 216,\n",
              " 'type': 217,\n",
              " 'capable': 218,\n",
              " 'performing': 219,\n",
              " 'contrast': 220,\n",
              " 'definition': 221,\n",
              " 'concept': 222,\n",
              " 'assigning': 223,\n",
              " 'probabilities': 224,\n",
              " 'sequences': 225,\n",
              " 'analysis': 226,\n",
              " 'corpora': 227,\n",
              " 'varying': 228,\n",
              " 'complexity': 229,\n",
              " 'n': 230,\n",
              " 'gram': 231,\n",
              " 'sophisticated': 232,\n",
              " 'network': 233,\n",
              " 'term': 234,\n",
              " 'usually': 235,\n",
              " 'number': 236,\n",
              " 'range': 237,\n",
              " 'millions': 238,\n",
              " 'billions': 239,\n",
              " 'patterns': 240,\n",
              " 'often': 241,\n",
              " 'indistinguishable': 242,\n",
              " 'built': 243,\n",
              " 'known': 244,\n",
              " 'typically': 245,\n",
              " 'run': 246,\n",
              " 'computer': 247,\n",
              " 'therefore': 248,\n",
              " 'provided': 249,\n",
              " 'service': 250,\n",
              " 'over': 251,\n",
              " 'api': 252,\n",
              " 'web': 253,\n",
              " 'interface': 254,\n",
              " 'vast': 255,\n",
              " 'sources': 256,\n",
              " 'books': 257,\n",
              " 'articles': 258,\n",
              " 'websites': 259,\n",
              " 'numerous': 260,\n",
              " 'forms': 261,\n",
              " 'content': 262,\n",
              " 'statistical': 263,\n",
              " 'phrases': 264,\n",
              " 'through': 265,\n",
              " 'training': 266,\n",
              " 'contextually': 267,\n",
              " 'chatgpt’s': 268,\n",
              " 'was': 269,\n",
              " 'possess': 270,\n",
              " 'knowledge': 271,\n",
              " 'diverse': 272,\n",
              " 'styles': 273,\n",
              " 'capabilities': 274,\n",
              " 'seem': 275,\n",
              " 'impressive': 276,\n",
              " 'surprising': 277,\n",
              " 'given': 278,\n",
              " 'functions': 279,\n",
              " 'operate': 280,\n",
              " 'special': 281,\n",
              " '“grammars”': 282,\n",
              " 'match': 283,\n",
              " 'up': 284,\n",
              " 'do': 285,\n",
              " 'here’s': 286,\n",
              " 'simplified': 287,\n",
              " 'explanation': 288,\n",
              " 'start': 289,\n",
              " 'reading': 290,\n",
              " 'amount': 291,\n",
              " 'giant': 292,\n",
              " 'library': 293,\n",
              " 'innovative': 294,\n",
              " 'unique': 295,\n",
              " 'structure': 296,\n",
              " 'remember': 297,\n",
              " 'down': 298,\n",
              " 'smaller': 299,\n",
              " 'pieces': 300,\n",
              " 'efficiently': 301,\n",
              " 'unlike': 302,\n",
              " 'programs': 303,\n",
              " 'individual': 304,\n",
              " 'relate': 305,\n",
              " 'picture': 306,\n",
              " 'getting': 307,\n",
              " 'specialized': 308,\n",
              " 'after': 309,\n",
              " 'particular': 310,\n",
              " 'subjects': 311,\n",
              " 'doing': 312,\n",
              " 'when': 313,\n",
              " 'you': 314,\n",
              " 'give': 315,\n",
              " 'prompt': 316,\n",
              " 'instruction': 317,\n",
              " 'they’ve': 318,\n",
              " 'learned': 319,\n",
              " 'respond': 320,\n",
              " 'having': 321,\n",
              " 'intelligent': 322,\n",
              " 'assistant': 323,\n",
              " 'toys': 324,\n",
              " 'poems': 325,\n",
              " 'music': 326,\n",
              " 'pictures': 327,\n",
              " 'invent': 328,\n",
              " 'stuff': 329,\n",
              " 'best': 330,\n",
              " 'builders': 331,\n",
              " 'they’re': 332,\n",
              " 'really': 333,\n",
              " 'stories': 334,\n",
              " 'translate': 335,\n",
              " 'write': 336,\n",
              " 'code': 337,\n",
              " 'so': 338,\n",
              " 'experts': 339,\n",
              " 'primarily': 340,\n",
              " 'consists': 341,\n",
              " 'networks': 342,\n",
              " 'together': 343,\n",
              " 'output': 344,\n",
              " 'layer': 345,\n",
              " 'converts': 346,\n",
              " 'high': 347,\n",
              " 'dimensional': 348,\n",
              " 'vector': 349,\n",
              " 'representation': 350,\n",
              " 'semantic': 351,\n",
              " 'syntactic': 352,\n",
              " 'fully': 353,\n",
              " 'connected': 354,\n",
              " 'apply': 355,\n",
              " 'nonlinear': 356,\n",
              " 'higher': 357,\n",
              " 'level': 358,\n",
              " 'abstractions': 359,\n",
              " 'designed': 360,\n",
              " 'interpret': 361,\n",
              " 'sequence': 362,\n",
              " 'maintain': 363,\n",
              " 'hidden': 364,\n",
              " 'updated': 365,\n",
              " 'step': 366,\n",
              " 'dependencies': 367,\n",
              " 'part': 368,\n",
              " 'allows': 369,\n",
              " 'focus': 370,\n",
              " 'selectively': 371,\n",
              " 'attend': 372,\n",
              " 'text’s': 373,\n",
              " 'most': 374,\n",
              " 'accurate': 375,\n",
              " 'examples': 376,\n",
              " 'let’s': 377,\n",
              " 'take': 378,\n",
              " 'openai': 379,\n",
              " '175': 380,\n",
              " 'bidirectional': 381,\n",
              " 'encoder': 382,\n",
              " 'representations': 383,\n",
              " 'transformers': 384,\n",
              " 'corpus': 385,\n",
              " 'meaningful': 386,\n",
              " 'xlnet': 387,\n",
              " 'carnegie': 388,\n",
              " 'mellon': 389,\n",
              " 'university': 390,\n",
              " 'uses': 391,\n",
              " 'novel': 392,\n",
              " '“permutation': 393,\n",
              " '”': 394,\n",
              " 'achieved': 395,\n",
              " 'art': 396,\n",
              " 'performance': 397,\n",
              " 'transfer': 398,\n",
              " 'variety': 399,\n",
              " 'translating': 400,\n",
              " 'creating': 401,\n",
              " 'summary': 402,\n",
              " 'robustly': 403,\n",
              " 'optimized': 404,\n",
              " 'pretraining': 405,\n",
              " 'facebook': 406,\n",
              " 'improved': 407,\n",
              " 'version': 408,\n",
              " 'performs': 409,\n",
              " 'easier': 410,\n",
              " 'developers': 411,\n",
              " 'applications': 412,\n",
              " 'leverage': 413,\n",
              " 'power': 414,\n",
              " 'free': 415,\n",
              " 'first': 416,\n",
              " 'multilingual': 417,\n",
              " 'transparency': 418,\n",
              " 'collaboration': 419,\n",
              " 'ever': 420,\n",
              " 'involved': 421,\n",
              " 'project': 422,\n",
              " '176': 423,\n",
              " 'larger': 424,\n",
              " 'than': 425,\n",
              " 'openai’s': 426,\n",
              " '46': 427,\n",
              " '13': 428,\n",
              " 'programming': 429,\n",
              " '1': 430,\n",
              " '6tb': 431,\n",
              " '320': 432,\n",
              " 'times': 433,\n",
              " 'works': 434,\n",
              " 'shakespeare': 435,\n",
              " 'recent': 436,\n",
              " 'years': 437,\n",
              " 'interest': 438,\n",
              " 'chatgpt': 439,\n",
              " 'very': 440,\n",
              " 'little': 441,\n",
              " 'seen': 442,\n",
              " 'breakthrough': 443,\n",
              " 'artificial': 444,\n",
              " 'intelligence': 445,\n",
              " 'major': 446,\n",
              " 'concern': 447,\n",
              " 'disrupt': 448,\n",
              " 'replacing': 449,\n",
              " 'legal': 450,\n",
              " 'documents': 451,\n",
              " 'drafts': 452,\n",
              " 'customer': 453,\n",
              " 'support': 454,\n",
              " 'news': 455,\n",
              " 'blogs': 456,\n",
              " 'etc': 457,\n",
              " 'lead': 458,\n",
              " 'losses': 459,\n",
              " 'those': 460,\n",
              " 'whose': 461,\n",
              " 'easily': 462,\n",
              " 'note': 463,\n",
              " 'replacement': 464,\n",
              " 'workers': 465,\n",
              " 'simply': 466,\n",
              " 'tool': 467,\n",
              " 'people': 468,\n",
              " 'productive': 469,\n",
              " 'efficient': 470,\n",
              " 'also': 471,\n",
              " 'created': 472,\n",
              " 'increased': 473,\n",
              " 'efficiency': 474,\n",
              " 'productivity': 475,\n",
              " 'enabled': 476,\n",
              " 'services': 477,\n",
              " 'were': 478,\n",
              " 'previously': 479,\n",
              " 'consuming': 480,\n",
              " 'expensive': 481,\n",
              " 'develop': 482,\n",
              " 'ways': 483,\n",
              " 'personalized': 484,\n",
              " 'education': 485,\n",
              " 'healthcare': 486,\n",
              " 'plans': 487,\n",
              " 'leading': 488,\n",
              " 'patient': 489,\n",
              " 'student': 490,\n",
              " 'outcomes': 491,\n",
              " 'governments': 492,\n",
              " 'decisions': 493,\n",
              " 'insights': 494,\n",
              " 'conclusion': 495,\n",
              " 'advancements': 496,\n",
              " 'makes': 497,\n",
              " 'great': 498,\n",
              " 'alternative': 499,\n",
              " 'regular': 500,\n",
              " 'usage': 501,\n",
              " 'industries': 502,\n",
              " 'biases': 503,\n",
              " 'associated': 504,\n",
              " 'critical': 505,\n",
              " 'eye': 506,\n",
              " 'evaluate': 507,\n",
              " 'careful': 508,\n",
              " 'continued': 509,\n",
              " 'development': 510,\n",
              " 'bring': 511,\n",
              " 'positive': 512,\n",
              " 'changes': 513,\n",
              " 'but': 514,\n",
              " 'we': 515,\n",
              " 'should': 516,\n",
              " 'aware': 517,\n",
              " 'limitations': 518,\n",
              " 'key': 519,\n",
              " 'takeaways': 520,\n",
              " 'intent': 521,\n",
              " 'grammatically': 522,\n",
              " 'correct': 523,\n",
              " 'explores': 524,\n",
              " 'discusses': 525,\n",
              " 'gpt3': 526,\n",
              " 'hugging': 527,\n",
              " 'face': 528,\n",
              " 'apis': 529,\n",
              " 'helpful': 530,\n",
              " 'users': 531,\n",
              " 'expected': 532,\n",
              " 'revolutionize': 533,\n",
              " 'market': 534}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now we'll try to create a dataset from this text as a supervised learning task dataset, where there's an input and an output\n",
        "> ### for ex. if text = \"How are you all?\"\n",
        "===========================================\n",
        "> ### Input: **How**\n",
        "> ### Output: **are**\n",
        "===========================================\n",
        "> ### Input: **How are**\n",
        "> ### Output: **you** and so on..."
      ],
      "metadata": {
        "id": "GUz4ZfvTTl2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentence in cleaned_text.split(\"\\n\"):\n",
        "    # convert words into numbers\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    print(tokenized_sentence)\n",
        "\n",
        "    for i in range(1, len(tokenized_sentence)):\n",
        "        input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAC4gaFqSyFo",
        "outputId": "8e0282c1-1c59-4b3e-bd75-a06a04783dba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[124, 11, 5, 10, 6, 13, 33]\n",
            "[5, 10, 6, 13, 11, 59, 216, 217, 3, 6, 13, 12, 11, 25, 60, 125, 48, 126, 16, 49, 61, 3, 7, 34, 19, 14, 23, 218, 3, 127, 128, 17, 7, 2, 219, 80, 50, 6, 81, 30]\n",
            "[8, 220, 1, 221, 3, 5, 6, 13, 129, 4, 1, 222, 3, 223, 224, 4, 225, 3, 26, 130, 16, 1, 226, 3, 7, 227, 5, 6, 13, 15, 20, 3, 228, 229, 21, 131, 230, 231, 14, 4, 51, 232, 132, 233, 14, 82, 1, 234, 133, 6, 134, 235, 129, 4, 14, 12, 62, 125, 48, 126, 2, 40, 5, 10, 236, 3, 83, 63, 15, 237, 21, 238, 4, 239, 19, 14, 15, 84, 135, 240, 8, 6, 2, 136, 7, 12, 11, 241, 242, 21, 12, 85, 18, 86]\n",
            "[64, 5, 10, 6, 13, 33, 11, 243]\n",
            "[5, 10, 137, 41, 13, 244, 52, 5, 133, 6, 134, 11, 245, 138, 49, 4, 246, 16, 5, 139, 247, 2, 11, 248, 249, 52, 5, 250, 251, 59, 252, 35, 253, 254, 19, 14, 23, 25, 16, 255, 61, 3, 7, 34, 21, 256, 140, 52, 257, 258, 259, 2, 260, 141, 261, 3, 85, 262, 18, 142, 1, 263, 143, 65, 26, 264, 2, 66, 265, 36, 266, 144, 1, 14, 15, 28, 145, 2, 267, 146, 147, 4, 148, 35, 149]\n",
            "[268, 53, 37, 5, 10, 6, 13, 269, 25, 16, 49, 61, 3, 150, 7, 34, 87, 24, 4, 29, 80, 67, 2, 270, 271, 3, 272, 151, 52, 5, 152, 24, 15, 136, 7, 8, 88, 273, 89, 90, 274, 68, 153, 7, 154, 2, 69, 70, 91, 275, 276, 31, 23, 155, 277, 278, 12, 19, 279, 280, 60, 281, 282, 12, 283, 284, 32, 148]\n",
            "[64, 285, 10, 6, 14, 38]\n",
            "[10, 6, 14, 17, 53, 37, 54, 156, 25, 41, 37, 38, 130, 16, 5, 41, 55, 286, 5, 287, 288, 3, 64, 31, 38]\n",
            "[48, 21, 92, 3, 7, 19, 14, 289, 18, 290, 5, 49, 291, 3, 7, 21, 1, 150, 157, 17, 48, 21, 5, 292, 293, 3, 71]\n",
            "[294, 55, 31, 62, 5, 295, 296, 158, 5, 41, 63, 93, 72, 29, 2, 297, 92, 3, 71]\n",
            "[159, 298, 26, 31, 160, 42, 66, 8, 299, 94, 17, 159, 26, 161, 300, 36, 93, 72, 38, 32, 6, 51, 301]\n",
            "[162, 26, 8, 66, 302, 131, 303, 19, 14, 29, 304, 26, 2, 64, 26, 305, 4, 95, 141, 8, 5, 96, 31, 163, 1, 164, 306]\n",
            "[307, 308, 309, 1, 165, 48, 31, 15, 20, 25, 51, 16, 166, 151, 4, 163, 167, 42, 168, 169, 17, 70, 97, 35, 170, 43, 310, 311]\n",
            "[312, 30, 313, 314, 315, 72, 5, 316, 5, 69, 35, 317, 31, 62, 124, 318, 319, 4, 320, 157, 17, 321, 59, 322, 323, 12, 15, 29, 2, 28, 7]\n",
            "[171, 65, 10, 6, 14, 2, 54, 44]\n",
            "[54, 44, 11, 17, 5, 172, 73, 32, 92, 3, 173, 324, 27, 174, 45, 169, 24, 15, 98, 325, 326, 327, 175, 328, 45, 329]\n",
            "[10, 6, 14, 23, 17, 1, 330, 176, 331, 8, 12, 73, 332, 333, 167, 42, 60, 26, 4, 177, 334, 335, 67, 178, 97, 2, 175, 336, 337]\n",
            "[338, 54, 44, 11, 1, 164, 73, 2, 9, 23, 1, 6, 339, 8, 12, 73]\n",
            "[165, 55]\n",
            "[1, 55, 3, 10, 6, 13, 340, 341, 3, 88, 22, 3, 132, 342, 17, 99, 22, 100, 22, 101, 22, 2, 102, 22, 19, 22, 38, 343, 4, 144, 1, 39, 7, 2, 28, 344, 179]\n",
            "[1, 101, 345, 346, 95, 176, 8, 1, 39, 7, 161, 5, 347, 348, 349, 350, 19, 180, 84, 351, 2, 352, 71, 43, 1, 26, 2, 74, 1, 13, 4, 29, 1, 103]\n",
            "[1, 100, 22, 3, 10, 6, 14, 40, 88, 353, 354, 22, 12, 355, 356, 181, 4, 1, 39, 180, 19, 22, 74, 1, 13, 182, 357, 358, 359, 21, 1, 39, 7]\n",
            "[1, 99, 22, 3, 9, 23, 360, 4, 361, 71, 21, 1, 39, 7, 8, 362, 19, 22, 363, 5, 364, 183, 12, 11, 365, 42, 95, 104, 366, 87, 1, 13, 4, 84, 1, 367, 65, 26, 8, 5, 96]\n",
            "[1, 102, 184, 11, 105, 106, 368, 3, 9, 63, 369, 1, 13, 4, 370, 371, 16, 173, 94, 3, 1, 39, 7, 36, 184, 93, 1, 13, 372, 4, 1, 39, 373, 374, 146, 94, 2, 28, 51, 375, 179]\n",
            "[376, 3, 9]\n",
            "[377, 378, 5, 160, 42, 75, 107, 10, 6, 14, 33]\n",
            "[53, 37, 54, 156, 25, 41, 37, 56, 36, 11, 108, 3, 1, 185, 10, 6, 14, 57, 18, 379, 24, 46, 380, 186, 83, 2, 15, 109, 187, 30, 68, 7, 110, 153, 2, 154]\n",
            "[47, 381, 382, 383, 21, 384, 56, 57, 18, 111, 47, 11, 105, 107, 33, 12, 46, 188, 25, 16, 5, 49, 385, 3, 7, 34, 24, 15, 29, 1, 103, 3, 5, 96, 2, 28, 386, 147, 4, 97]\n",
            "[387, 56, 36, 33, 57, 18, 388, 389, 390, 2, 111, 391, 5, 392, 112, 4, 6, 189, 158, 393, 6, 189, 394, 24, 46, 395, 183, 3, 1, 396, 397, 16, 6, 30, 68, 6, 110, 2, 69, 70]\n",
            "[190, 7, 4, 7, 398, 41, 56, 190, 57, 18, 111, 11, 25, 16, 5, 399, 3, 6, 30, 2, 15, 109, 7, 4, 7, 181, 17, 400, 7, 4, 105, 6, 401, 5, 402, 2, 69, 70]\n",
            "[113, 403, 404, 47, 405, 112, 56, 57, 18, 406, 44, 191, 113, 11, 59, 407, 47, 408, 12, 409, 114, 16, 115, 6, 30]\n",
            "[116, 117, 10, 6, 13, 33]\n",
            "[1, 192, 3, 116, 117, 9, 46, 193, 1, 118, 3, 50, 6, 81, 174, 24, 410, 27, 194, 411, 2, 119, 4, 195, 412, 12, 413, 1, 414, 3, 19, 14, 4, 195, 196, 42, 137, 27, 415, 108, 140, 120, 11, 76, 24, 11, 1, 416, 417, 10, 6, 13, 33, 25, 8, 197, 418, 18, 1, 185, 419, 3, 44, 194, 420, 421, 8, 5, 139, 191, 422]\n",
            "[32, 90, 423, 186, 83, 424, 425, 426, 53, 37, 76, 15, 28, 7, 8, 427, 50, 67, 2, 428, 429, 67, 24, 11, 25, 16, 430, 431, 3, 7, 34, 432, 433, 1, 197, 434, 3, 435]\n",
            "[198, 121, 3, 9]\n",
            "[8, 436, 437, 122, 46, 188, 166, 438, 8, 10, 6, 13, 9, 17, 53, 37, 2, 199, 17, 439, 63, 15, 28, 50, 6, 7, 12, 46, 440, 441, 171, 21, 12, 85, 18, 86, 89, 9, 40, 442, 5, 443, 8, 1, 118, 3, 444, 445, 44, 122, 23, 200, 43, 58, 123, 16, 77, 201, 202, 2, 78]\n",
            "[108, 446, 447, 43, 9, 11, 58, 79, 4, 448, 77, 201, 10, 6, 13, 32, 104, 203, 20, 204, 4, 109, 30, 18, 449, 86, 17, 450, 451, 2, 452, 453, 454, 199, 170, 455, 456, 457, 36, 205, 458, 4, 77, 459, 27, 460, 461, 38, 15, 20, 462, 206]\n",
            "[82, 24, 11, 106, 4, 463, 12, 9, 23, 155, 5, 464, 27, 128, 465, 31, 23, 466, 5, 467, 12, 15, 74, 468, 4, 20, 51, 469, 2, 470, 8, 58, 38, 89, 75, 207, 91, 20, 206, 45, 207, 203, 471, 20, 472, 52, 5, 152, 3, 1, 473, 474, 2, 475, 476, 18, 9, 27, 120, 119, 91, 20, 204, 4, 98, 45, 196, 35, 477, 12, 478, 479, 138, 104, 480, 35, 481, 4, 482]\n",
            "[9, 40, 1, 79, 4, 123, 78, 8, 115, 483, 27, 120, 9, 205, 20, 208, 4, 98, 484, 485, 35, 486, 487, 488, 4, 114, 489, 2, 490, 491, 9, 15, 20, 208, 4, 74, 119, 2, 492, 177, 114, 493, 18, 142, 10, 61, 3, 34, 2, 127, 494]\n",
            "[495]\n",
            "[10, 6, 13, 9, 40, 193, 1, 118, 3, 50, 6, 81, 87, 27, 45, 496, 8, 7, 110, 2, 162, 9, 15, 182, 21, 172, 34, 29, 90, 103, 2, 209, 2, 178, 210, 149, 36, 497, 72, 5, 498, 499, 27, 500, 501, 8, 80, 30, 8, 115, 502, 82, 122, 23, 200, 43, 1, 211, 121, 2, 79, 503, 504, 32, 19, 14, 24, 11, 106, 4, 112, 9, 32, 5, 505, 506, 2, 507, 58, 123, 16, 78, 32, 508, 62, 2, 509, 510, 9, 40, 1, 79, 4, 511, 43, 512, 513, 8, 187, 212, 514, 515, 516, 20, 517, 3, 58, 518, 2, 211, 121]\n",
            "[519, 520]\n",
            "[10, 6, 14, 9, 15, 29, 135, 66, 29, 143, 65, 209, 2, 210, 521, 2, 28, 45, 7, 12, 11, 145, 2, 522, 523]\n",
            "[1, 213, 524, 1, 55, 3, 75, 9, 68, 101, 100, 99, 2, 102, 22]\n",
            "[1, 213, 525, 75, 3, 1, 107, 9, 17, 47, 47, 76, 2, 526, 2, 1, 192, 3, 116, 117, 9]\n",
            "[527, 528, 529, 15, 20, 530, 27, 531, 4, 28, 7, 60, 9, 17, 214, 10, 215, 113, 76, 2, 214, 10, 215]\n",
            "[9, 23, 532, 4, 533, 168, 212, 8, 1, 77, 534, 202, 2, 78, 8, 1, 198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the input shape must be same when passing the sequences into the neural networks, so for that we will applying **padding** in front of every sentence with respect to which sentence has max. number of words."
      ],
      "metadata": {
        "id": "TAZj2P2HgJnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(x) for x in input_sequences])\n",
        "max_length # there present a sentence which has maximum 111 words."
      ],
      "metadata": {
        "id": "OtGMB4NqdOYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfc72be-8295-4143-ddfe-b0fa0756a8fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now we'll apply **zero_padding**"
      ],
      "metadata": {
        "id": "_LkltLXohC6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "PTQTuQ32f3dQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre')"
      ],
      "metadata": {
        "id": "UwGq33BqzIq7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtDnNxWghhfU",
        "outputId": "c91ca30d-0f61-425a-b23a-51ff15325b26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0, 124,  11],\n",
              "       [  0,   0,   0, ..., 124,  11,   5],\n",
              "       [  0,   0,   0, ...,  11,   5,  10],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   2,  78,   8],\n",
              "       [  0,   0,   0, ...,  78,   8,   1],\n",
              "       [  0,   0,   0, ...,   8,   1, 198]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]\n",
        "y = padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "hGlBl6FQhsFM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB13TA8kiD9E",
        "outputId": "e6153abf-003e-4999-f17f-73f92ed44b85"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0, 124],\n",
              "       [  0,   0,   0, ...,   0, 124,  11],\n",
              "       [  0,   0,   0, ..., 124,  11,   5],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 202,   2,  78],\n",
              "       [  0,   0,   0, ...,   2,  78,   8],\n",
              "       [  0,   0,   0, ...,  78,   8,   1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rit_y8_AiEa5",
        "outputId": "557041aa-97b9-4c8f-d50a-09cccd1545ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 11,   5,  10, ...,   8,   1, 198], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now from our dataset (X and y), as we have **discrete values**, we can use **multi-class classification.**"
      ],
      "metadata": {
        "id": "FOfaokwOkeui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVWOTRdWj9S7",
        "outputId": "78a0f37c-0bfc-4c20-f9e6-04313cba542b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1478, 110), (1478,))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### X.shape = (1478, 110) => means **in each sentence there are 110 words** and there are **total 1478 sentences**."
      ],
      "metadata": {
        "id": "rjFmOA_lDcou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now we'll **ONE HOT ENCODE** the y, which is currently a scaler."
      ],
      "metadata": {
        "id": "5hG4BaePlabZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index)+1)\n",
        "# +1 because OHE starts from 0 and word_index returned output started from 1"
      ],
      "metadata": {
        "id": "Xu4IgH-1lJ0V"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape     # (1478, 535), 535 because there are total 535 words in our vocabulary."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXsqWBbVlzl9",
        "outputId": "272696ad-07b3-41ea-f215-8c35f1ef97d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1478, 535)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y   # each sentence is represented by a sparse vector having 535 values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFnle3xOmcM1",
        "outputId": "0e44c9bf-4d61-4e56-fee1-9a139e6b6da9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we're reading with **TRAINING THE DATA**"
      ],
      "metadata": {
        "id": "L6V8VmKAmsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "NwwJUxTimr7P"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=535, output_dim=100))\n",
        "# there are total 535 words in our vocabulary and 110 is the length of each sentence\n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(units=535, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor=\"accuracy\", patience=5, restore_best_weights=True)\n",
        "history = model.fit(X, y, epochs=50, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOvBN3fFmowp",
        "outputId": "38c9b516-7309-40e2-efc4-810457865849"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 197ms/step - accuracy: 0.0252 - loss: 6.1211\n",
            "Epoch 2/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 194ms/step - accuracy: 0.0429 - loss: 5.4428\n",
            "Epoch 3/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 170ms/step - accuracy: 0.0982 - loss: 4.8814\n",
            "Epoch 4/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 195ms/step - accuracy: 0.2134 - loss: 4.0292\n",
            "Epoch 5/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 197ms/step - accuracy: 0.3455 - loss: 3.0199\n",
            "Epoch 6/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 188ms/step - accuracy: 0.5531 - loss: 1.9757\n",
            "Epoch 7/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 176ms/step - accuracy: 0.7035 - loss: 1.2912\n",
            "Epoch 8/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 196ms/step - accuracy: 0.8642 - loss: 0.7032\n",
            "Epoch 9/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 195ms/step - accuracy: 0.9224 - loss: 0.4159\n",
            "Epoch 10/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 198ms/step - accuracy: 0.9539 - loss: 0.2502\n",
            "Epoch 11/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.9730 - loss: 0.1621\n",
            "Epoch 12/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 167ms/step - accuracy: 0.9827 - loss: 0.0969\n",
            "Epoch 13/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 196ms/step - accuracy: 0.9857 - loss: 0.0725\n",
            "Epoch 14/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 198ms/step - accuracy: 0.9818 - loss: 0.0723\n",
            "Epoch 15/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 174ms/step - accuracy: 0.9906 - loss: 0.0497\n",
            "Epoch 16/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 167ms/step - accuracy: 0.9879 - loss: 0.0483\n",
            "Epoch 17/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 195ms/step - accuracy: 0.9879 - loss: 0.0438\n",
            "Epoch 18/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 194ms/step - accuracy: 0.9819 - loss: 0.0621\n",
            "Epoch 19/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 180ms/step - accuracy: 0.9880 - loss: 0.0435\n",
            "Epoch 20/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 169ms/step - accuracy: 0.9884 - loss: 0.0444\n",
            "Epoch 21/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 215ms/step - accuracy: 0.9848 - loss: 0.0501\n",
            "Epoch 22/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 264ms/step - accuracy: 0.9888 - loss: 0.0389\n",
            "Epoch 23/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 198ms/step - accuracy: 0.9899 - loss: 0.0329\n",
            "Epoch 24/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 189ms/step - accuracy: 0.9819 - loss: 0.0419\n",
            "Epoch 25/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.9849 - loss: 0.0404\n",
            "Epoch 26/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 168ms/step - accuracy: 0.9876 - loss: 0.0385\n",
            "Epoch 27/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 194ms/step - accuracy: 0.9835 - loss: 0.0384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowing the summary of the model..."
      ],
      "metadata": {
        "id": "T1ThjNg--KtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "DlYmybJBrFrL",
        "outputId": "a73a73f8-6b9c-4d84-e090-50c7f1f79ffa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │          \u001b[38;5;34m53,500\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m535\u001b[0m)                 │          \u001b[38;5;34m69,015\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">53,500</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">535</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">69,015</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m719,291\u001b[0m (2.74 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">719,291</span> (2.74 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m239,763\u001b[0m (936.57 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">239,763</span> (936.57 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m479,528\u001b[0m (1.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">479,528</span> (1.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's plot accuracy v/s loss curve"
      ],
      "metadata": {
        "id": "sYVkpqPh-VcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_1 = pd.DataFrame(history.history)\n",
        "plt.figure(dpi=150, figsize = (5,3))\n",
        "plt.plot(plot_1)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(plot_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "Qcp1n9gvekcX",
        "outputId": "75aff455-279a-49b7-bbf1-05efeaf883e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7b92a6bdf5e0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x450 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally let's watch the output on giving **LLMs are** as an input..."
      ],
      "metadata": {
        "id": "B3E648yV-a5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "z-V0rPiU68l5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"LLMs are\"\n",
        "\n",
        "for i in range(10):\n",
        "    # tokenize\n",
        "    token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "    # padding\n",
        "    padded_token_text = pad_sequences([token_text], maxlen=108, padding='pre')\n",
        "\n",
        "    # predict\n",
        "    position = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "    # print word at the particular position in tokenizer.word_index\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == position:\n",
        "            text = text + \" \" + word\n",
        "            print(text)\n",
        "            time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88sCwok_24ps",
        "outputId": "a56a63a0-38d7-46aa-b796-3c66cf71e5d1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
            "LLMs are expected\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "LLMs are expected to\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "LLMs are expected to revolutionize\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "LLMs are expected to revolutionize certain\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "LLMs are expected to revolutionize certain domains\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "LLMs are expected to revolutionize certain domains in\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "LLMs are expected to revolutionize certain domains in the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "LLMs are expected to revolutionize certain domains in the job\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "LLMs are expected to revolutionize certain domains in the job market\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "LLMs are expected to revolutionize certain domains in the job market communication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I think it's performing well :)"
      ],
      "metadata": {
        "id": "brxH57Ho-mz_"
      }
    }
  ]
}